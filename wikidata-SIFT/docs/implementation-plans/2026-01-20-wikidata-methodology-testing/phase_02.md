# Wikidata Methodology Testing Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use ed3d-plan-and-execute:executing-an-implementation-plan to implement this plan task-by-task.

**Goal:** Set up workflow and documentation for Stage 1 testing execution

**Architecture:** Human verification workflow with structured guides, checklists, and results templates

**Tech Stack:** Markdown documentation, YAML logs

**Scope:** 5 phases from original design (Phase 2 of 5)

**Codebase verified:** 2026-01-20

---

## Phase 2: Stage 1 Testing Execution Setup

**Goal:** Create documentation and workflow for running 200+ claim verifications

### Task 1: Create Human Verification Guide

**Files:**
- Create: `docs/human-verification-guide.md`

**Step 1: Create the guide**

Create `docs/human-verification-guide.md`:

```markdown
# Human Verification Guide for SIFT Methodology Testing

## Purpose

This guide explains how to verify proposed claims generated by the SIFT methodology testing skill. Your verification is the ground truth for measuring accuracy.

## Verification Workflow

### 1. Review the Proposed Claim

Each log file in `logs/wikidata-methodology-testing/` contains a proposed claim. Review:

- **Entity:** Is this the correct entity?
- **Property:** Is this an appropriate property for this entity?
- **Proposed Value:** Does this value seem plausible?
- **Confidence:** Does the stated confidence match your assessment?

### 2. Verify Sources

For each source in `sources_consulted`:

1. **Open the URL** - Does it load? Is it the correct page?
2. **Find the claim** - Does the source actually contain information about this property?
3. **Check accuracy** - Does the source support the proposed value?

**Common issues:**
- URL returns 404 (hallucinated source)
- URL exists but doesn't mention the entity (wrong source)
- Source mentions entity but not this property (misread source)
- Source has different value than proposed (incorrect value)

### 3. Verify SIFT Analysis

Review the `sift_verification` section:

- **Stop:** Did the analysis question appropriate assumptions?
- **Investigate:** Is the source assessment accurate?
- **Find:** Were cross-references actually found and consulted?
- **Trace:** Was a primary source identified (if applicable)?

### 4. Fill in Human Verification

Update the `human_verification` section of the YAML file:

```yaml
human_verification:
  reviewed_by: "[your name]"
  review_date: "[YYYY-MM-DD]"
  sift_correct: [true|false]  # Was the SIFT analysis correct?
  proposed_value_correct: [true|false]  # Is the proposed value correct?
  actual_value: "[value]"  # If different from proposed, what's correct?
  failure_mode: "[mode]"  # If incorrect, why? (see taxonomy below)
  notes: "[any additional notes]"
```

## Failure Mode Taxonomy

When `sift_correct: false` or `proposed_value_correct: false`, classify the failure:

| Failure Mode | Description | Example |
|--------------|-------------|---------|
| `hallucinated_source` | Source URL doesn't exist or returns 404 | LLM cited a URL that was never real |
| `wrong_source` | Source exists but is about different entity/topic | Cited a page about a different person with same name |
| `misread_source` | Source exists but doesn't support the claim | Source mentions entity but not this property |
| `incorrect_value` | Source supports a claim but with different value | Source says 1952, LLM proposed 1953 |
| `wrong_property` | Proposed wrong property for this type of information | Used P569 (birth) when should be P571 (inception) |
| `insufficient_precision` | Value correct but precision wrong | Source says "1950s", LLM claimed specific date |
| `outdated_source` | Source was accurate but information has changed | Company HQ moved since source was published |
| `conflicting_sources` | Sources disagree and LLM chose wrong one | Two sources give different dates |
| `primary_source_missing` | Claimed primary source but it wasn't traced | Said "per official records" but cited Wikipedia |
| `other` | Doesn't fit above categories | Document in notes |

## Verification Standards

### What counts as "SIFT correct"?

SIFT is correct if:
- Sources cited actually exist and are accessible
- Sources actually contain information about the claimed property
- Source assessment (reliability, type) is reasonable
- Cross-referencing was attempted where appropriate
- Primary sources were traced where available

SIFT is incorrect if:
- Any source is hallucinated
- Source assessment is significantly wrong
- Claims about cross-referencing are false
- Primary source claim is unfounded

### What counts as "value correct"?

Value is correct if:
- The proposed value matches what reliable sources say
- Precision is appropriate (don't claim day precision if source only gives year)
- Value type is correct (item vs string vs date)

Value is incorrect if:
- Sources give a different value
- Value is an extrapolation beyond what sources support
- Precision is overstated

## Tips for Efficient Verification

1. **Batch by entity type** - Verify all humans, then all orgs, then all works
2. **Keep source tabs open** - Often multiple claims cite same sources
3. **Use Wikidata as cross-reference** - Check what Wikidata already has for the entity
4. **Note patterns** - If you see the same failure mode repeatedly, note it
5. **Time yourself** - Track how long verification takes for later analysis

## Questions?

If you encounter a case that doesn't fit this guide, document it thoroughly in the notes field and flag for discussion.
```

**Step 2: Commit**

```bash
git add docs/human-verification-guide.md
git commit -m "docs: add human verification guide for SIFT testing"
```

---

### Task 2: Create Test Execution Checklist

**Files:**
- Create: `docs/test-execution-checklist.md`

**Step 1: Create the checklist**

Create `docs/test-execution-checklist.md`:

```markdown
# Test Execution Checklist

## Pre-Testing Setup

- [ ] Phase 1 infrastructure complete (skill, entities, analysis script)
- [ ] Verified pywikibot can read from production Wikidata
- [ ] Chainlink session started for tracking

## Stage 1 Testing (200+ claims)

### Batch 1: Humans (20 entities × ~3-5 properties = 60-100 claims)

For each human entity in `docs/test-entities.yaml`:

```bash
# Run the testing skill
/wikidata-methodology-testing [Q-id]
```

- [ ] Q42 (Douglas Adams) - properties verified, logged
- [ ] Q5879 (Goethe) - properties verified, logged
- [ ] Q937 (Einstein) - properties verified, logged
- [ ] Q7186 (Marie Curie) - properties verified, logged
- [ ] Q1339 (Bach) - properties verified, logged
- [ ] Q76 (Obama) - properties verified, logged
- [ ] Q317521 (Musk) - properties verified, logged
- [ ] Q6701196 (Luis Villa) - properties verified, logged
- [ ] Q313590 (Mitchell Baker) - properties verified, logged
- [ ] Q2845707 (Karen Sandler) - properties verified, logged
- [ ] Q7296746 (Raph Levien) - properties verified, logged
- [ ] Q4039475 (Bradley Kuhn) - properties verified, logged
- [ ] Q36215 (Tim Berners-Lee) - properties verified, logged
- [ ] Q92743 (Vint Cerf) - properties verified, logged
- [ ] Q2387325 (Moxie Marlinspike) - properties verified, logged
- [ ] Q17174219 (Limor Fried) - properties verified, logged
- [ ] Q553790 (Aaron Swartz) - properties verified, logged
- [ ] Q21540213 (Deb Nicholson) - properties verified, logged
- [ ] Q4723060 (Allison Randal) - properties verified, logged
- [ ] Q16195460 (Asheesh Laroia) - properties verified, logged

Human verification complete for batch 1: [ ]

### Batch 2: Organizations (20 entities × ~3-5 properties = 60-100 claims)

- [ ] Q312 (Apple) - properties verified, logged
- [ ] Q95 (Google) - properties verified, logged
- [ ] Q380 (Meta) - properties verified, logged
- [ ] Q9531 (BBC) - properties verified, logged
- [ ] Q8525 (Mozilla) - properties verified, logged
- [ ] Q170877 (Wikimedia) - properties verified, logged
- [ ] Q737 (NASA) - properties verified, logged
- [ ] Q5396743 (EFF) - properties verified, logged
- [ ] Q1093914 (OSI) - properties verified, logged
- [ ] Q671779 (FSF) - properties verified, logged
- [ ] Q99658699 (Tidelift) - properties verified, logged
- [ ] Q55597695 (Conservancy) - properties verified, logged
- [ ] Q40561 (Linux Foundation) - properties verified, logged
- [ ] Q7414033 (GNOME Foundation) - properties verified, logged
- [ ] Q193489 (Creative Commons) - properties verified, logged
- [ ] Q7071698 (OIN) - properties verified, logged
- [ ] Q30089773 (OpenSSF) - properties verified, logged
- [ ] Q7097920 (Open Rights Group) - properties verified, logged
- [ ] Q5193377 (Courage Foundation) - properties verified, logged
- [ ] Q105576557 (OpenSSF duplicate?) - properties verified, logged

Human verification complete for batch 2: [ ]

### Batch 3: Creative Works (20 entities × ~3-5 properties = 60-100 claims)

- [ ] Q25338 (Hitchhiker's Guide) - properties verified, logged
- [ ] Q47209 (1984) - properties verified, logged
- [ ] Q208460 (Lord of the Rings) - properties verified, logged
- [ ] Q184843 (Catcher in the Rye) - properties verified, logged
- [ ] Q170583 (Star Wars) - properties verified, logged
- [ ] Q388 (Linux) - properties verified, logged
- [ ] Q82580 (Firefox) - properties verified, logged
- [ ] Q11015 (Python) - properties verified, logged
- [ ] Q2005 (Wikipedia) - properties verified, logged
- [ ] Q131339 (Git) - properties verified, logged
- [ ] Q14813 (GIMP) - properties verified, logged
- [ ] Q188893 (Blender) - properties verified, logged
- [ ] Q846045 (LibreOffice) - properties verified, logged
- [ ] Q10287 (Emacs) - properties verified, logged
- [ ] Q15206305 (Signal) - properties verified, logged
- [ ] Q125977 (GPL) - properties verified, logged
- [ ] Q17061486 (DFSG) - properties verified, logged
- [ ] Q854449 (OSD) - properties verified, logged
- [ ] Q341 (Cathedral and Bazaar) - properties verified, logged
- [ ] Q1630105 (Revolution OS) - properties verified, logged

Human verification complete for batch 3: [ ]

## Post-Testing Analysis

- [ ] Run analysis script: `python scripts/analyze_test_results.py`
- [ ] Document results in `docs/test-results-stage1.md`
- [ ] Identify top failure modes
- [ ] Calculate accuracy by entity type and property
- [ ] Make go/no-go recommendation

## Metrics to Record

After each batch, note:
- Total claims proposed
- Total claims verified
- Time spent on verification
- Common issues encountered
- Any patterns in failures
```

**Step 2: Commit**

```bash
git add docs/test-execution-checklist.md
git commit -m "docs: add test execution checklist"
```

---

### Task 3: Create Results Template

**Files:**
- Create: `docs/test-results-stage1.md`

**Step 1: Create the results template**

Create `docs/test-results-stage1.md`:

```markdown
# Stage 1 Test Results

**Test Period:** [Start Date] - [End Date]
**Tested By:** [Name(s)]

## Summary

| Metric | Value |
|--------|-------|
| Total entities tested | /60 |
| Total claims proposed | |
| Total claims verified | |
| SIFT accuracy | % |
| Value accuracy | % |
| Average verification time per claim | min |

## Go/No-Go Assessment

**SIFT Accuracy:** [X]%

- [ ] ≥99% - Autonomous operation viable
- [ ] 95-99% - Consider confidence-based filtering
- [ ] <95% - Methodology needs iteration

**Recommendation:** [PROCEED / ITERATE / STOP]

## Accuracy by Entity Type

| Entity Type | Claims | Correct | Accuracy |
|-------------|--------|---------|----------|
| Human | | | % |
| Organization | | | % |
| Creative Work | | | % |

## Accuracy by Property Type

| Property | Claims | Correct | Accuracy |
|----------|--------|---------|----------|
| P569 (birth date) | | | % |
| P570 (death date) | | | % |
| P27 (citizenship) | | | % |
| P106 (occupation) | | | % |
| ... | | | % |

## Failure Mode Analysis

| Failure Mode | Count | % of Failures |
|--------------|-------|---------------|
| hallucinated_source | | % |
| misread_source | | % |
| incorrect_value | | % |
| wrong_property | | % |
| ... | | % |

## Notable Findings

### Patterns Observed

1. [Pattern 1]
2. [Pattern 2]

### Difficult Cases

1. [Case 1 - why it was hard]
2. [Case 2 - why it was hard]

### Methodology Improvements Suggested

1. [Suggestion 1]
2. [Suggestion 2]

## Raw Data

Full test logs available in `logs/wikidata-methodology-testing/`

Analysis generated by: `python scripts/analyze_test_results.py`

## Next Steps

Based on results:
- [ ] If ≥99%: Proceed to Stage 2 (confidence calibration)
- [ ] If 95-99%: Analyze high-confidence subset
- [ ] If <95%: Document failure patterns, iterate methodology
```

**Step 2: Commit**

```bash
git add docs/test-results-stage1.md
git commit -m "docs: add Stage 1 results template"
```

---

## Phase 2 Verification

**Done when:**
- [ ] Human verification guide at `docs/human-verification-guide.md`
- [ ] Test execution checklist at `docs/test-execution-checklist.md`
- [ ] Results template at `docs/test-results-stage1.md`
- [ ] All committed to git

**Note:** Actual test execution happens after Phase 2 infrastructure is in place. Phases 3-5 will be written after learnings from Stage 1 testing.
